{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFvBuTI5gWDr"
      },
      "source": [
        "# Трансформеры\n",
        "В этом домашнем задании мы рассмотим использование трансформеров в библиотеке PyTorch. Рассмотрим задачу языкового моделирования. Попробуем генерировать текст нейронной сетью. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-Ce9WRagWDu"
      },
      "source": [
        "Ссылка на данные - https://drive.google.com/drive/folders/1x1A4ElliUGBPnHladGMwPxPuGxI8Vnpu?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DsMWulR6gWDu"
      },
      "outputs": [],
      "source": [
        "# хороший тон, импортировать все необходимые библиотеки в одной ячейке ;)\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "from tqdm import trange, tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr0AVlwXgWDv"
      },
      "source": [
        "Что такое языковое моделирование? Это предсказание вероятности следующего токена (слова или буквы) на основе предыдущих токенов. Математически это можно описать так:\n",
        "\n",
        "$$P(x_i|x_1, x_2 , ... , x_{i-1})$$ \n",
        "\n",
        "Последовательность $$ x_1, x_2, ... x_{i-1} $$ называют контекстом."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHzbR69zgWDw"
      },
      "source": [
        "## Задание 0 (0 баллов, но сделать нужно)\n",
        "Проставьте знаки неравенств, исходя из вашего опыта:\n",
        "$$ P(раму | мама, мыла) * P(папу | мама, мыла) $$\n",
        "$$ P(столу | дорога, ложка, к) * P(обеду | дорога, ложка, к) $$\n",
        "$$ P(Евпатий | меня, зовут) * P(Ваня | меня, зовут) $$\n",
        "$$ P(журналы | я, часто ,читаю) * P(комиксы | я, часто ,читаю) $$\n",
        "Попробуйте объяснить выбор для каждого из примеров."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0MgIwdEgWDw"
      },
      "source": [
        "Ответ : > , так как есть такой стишок \n",
        "\n",
        "< , ложка к обеду звучит как-то логичнее\n",
        "\n",
        "< , Ваня более часто встречающееся имя чем Евпатий\n",
        "\n",
        "< , для молодого поколения очевиднее комиксы, чем журналы"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6urg35wggWDw"
      },
      "source": [
        "Если для некоторых из примеров проставить знаки достаточно просто, то на некоторые сложно сказать, какой овтет верный. Мы принимаем решение для данного задания исходя их опыта использования русского языка. Мы много читали на русском и слушали огромное количество русской речи. Обучение языковых моделей происходит по схожему принципу. \n",
        "\n",
        "Мы хотим показать модели столько текстов, сколько можем и надеемся, что она наберется достаточно опыта, чтобы расставлять такие знаки неравества максимально схоже с человеком."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GLT0ISzgWDw"
      },
      "source": [
        "## Задание 1 (0.5 балла)\n",
        "Мы будем обучать языковую модель для предсказания следущей буквы. Такие языковые модели применяются в распозновании речи, так как предоставляют дополнительную информацию акустической модели при выборе следующего символа. Для начала, откройте файл с данными, посмотрите, какие символы входят в тексты, сколько их. Уберите из текста все символы переноса на новую строку и табуляцию."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1NukE6ggWDx",
        "outputId": "5d0c3635-5703-4f4c-997b-e469095a28b0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "700000"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "path = 'small_corp_for_test.txt'\n",
        "file = open(path, 'r')\n",
        "data = file.readlines()\n",
        "file.close()\n",
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "A5b06t95gWDy"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TIH_mQiAgWDy"
      },
      "outputs": [],
      "source": [
        "data = [text.split() for text in data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70sBZAXugWDy"
      },
      "outputs": [],
      "source": [
        "letters = list()\n",
        "for string in data:\n",
        "    for word in string:\n",
        "        word = list(word)\n",
        "        for char in word:\n",
        "            letters.append(char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOUpFT38gWDy"
      },
      "outputs": [],
      "source": [
        "my_dict = Counter(letters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eP3XLlGsgWDy",
        "outputId": "252fe38b-90f3-476c-88bf-28bc4301bb87"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'-': 43618,\n",
              "         'а': 2366842,\n",
              "         'б': 384499,\n",
              "         'в': 1130062,\n",
              "         'г': 402608,\n",
              "         'д': 899550,\n",
              "         'е': 2028774,\n",
              "         'ж': 249344,\n",
              "         'з': 355889,\n",
              "         'и': 1501660,\n",
              "         'й': 289713,\n",
              "         'к': 795405,\n",
              "         'л': 915552,\n",
              "         'м': 878018,\n",
              "         'н': 1582468,\n",
              "         'о': 2779376,\n",
              "         'п': 757949,\n",
              "         'р': 1055739,\n",
              "         'с': 1401746,\n",
              "         'т': 1985934,\n",
              "         'у': 848660,\n",
              "         'ф': 60354,\n",
              "         'х': 147781,\n",
              "         'ц': 117824,\n",
              "         'ч': 370993,\n",
              "         'ш': 187256,\n",
              "         'щ': 66697,\n",
              "         'ъ': 5406,\n",
              "         'ы': 403604,\n",
              "         'ь': 538119,\n",
              "         'э': 162631,\n",
              "         'ю': 149791,\n",
              "         'я': 555131,\n",
              "         'ё': 76849})"
            ]
          },
          "metadata": {},
          "execution_count": 192
        }
      ],
      "source": [
        "my_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-79zJxEZgWDz",
        "outputId": "51da9c68-b0e0-4b47-9e69-1f6c02858bce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of letters is 25495842\n"
          ]
        }
      ],
      "source": [
        "sum = 0\n",
        "for value in my_dict.values():\n",
        "    sum += value\n",
        "print(f\"Total number of letters is {sum}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9AP_n68gWDz"
      },
      "source": [
        "## Задание 2 (0.5 балла)\n",
        "Для обучения модели требуется сначала подготовить текст в подходящий для нейросети вид. Важно также отметить, что нужно добавить два токена start и end, которые отвечают за начало и конец текста. Используйте [ и ] для этой задачи. Также нам нужен токен pad, чтобы заполнять им текст до требуемой длинны для формирования батча.\n",
        "\n",
        "Реализуйте метод preprocess класса Preprocessor. Он должен принимать на вход текст и длинну текста, которую мы ожидаем получить на выходе. Текст должен быть переведен в нижний регистр, в конец текста добавляется требуемое число pad токенов, далее текст векторизуется (каждому символу ставится свое число). Вернуть требуется два вектора. Полученный результат без последнего токена (на нем будем обучаться) и полученный результат без первого токена (целевые метки при обучении)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zj19rsW8gWD0"
      },
      "outputs": [],
      "source": [
        "class Preprocessor:\n",
        "    def __init__(self):\n",
        "        self.alphabet = '_добсркгаупитнезчмфяжлйвцыэь-шхющёъ][ '\n",
        "        self.token2ind = {}\n",
        "        self.ind2token = {}\n",
        "        for i in range(len(self.alphabet)):\n",
        "            self.token2ind[self.alphabet[i]] = i\n",
        "            self.ind2token[i] = self.alphabet[i]\n",
        "        \n",
        "    \n",
        "    def preprocess(self, text, window_size):\n",
        "        answer = []\n",
        "        text = text.lower() + \"_\" * (window_size - len(text))\n",
        "        for symbol in text:\n",
        "            answer.append(self.token2ind[symbol])\n",
        "        return np.array(answer[:-1]), np.array(answer[1:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTqkUpkKgWD0"
      },
      "source": [
        "## Задание 3 (0.5 балла)\n",
        "Так как мы решили, что текст будет начинаться токеном [ и заканчиваться токеном ], данные нужно поправить. Реализуйте эту идею, добавьте данные токены в ваши тексты."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "R8IA1o3ngWD0"
      },
      "outputs": [],
      "source": [
        "for i in range(len(data)):\n",
        "    data[i] = '[' + \" \".join(data[i]) + ']'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lt9IkQBAgWD1",
        "outputId": "aa1876ca-dedc-44d7-8570-65aa24bac48f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[добро]',\n",
              " '[кого]',\n",
              " '[капитан]',\n",
              " '[нет]',\n",
              " '[зачем]',\n",
              " '[что происходит]',\n",
              " '[что такое]',\n",
              " '[рассказ]',\n",
              " '[никому]',\n",
              " '[ну что]',\n",
              " '[кто]',\n",
              " '[я укажу]',\n",
              " '[исполняй]',\n",
              " '[ждет]',\n",
              " '[он думал]',\n",
              " '[в броне]',\n",
              " '[отец]',\n",
              " '[быстро]',\n",
              " '[ну жил]',\n",
              " '[что]',\n",
              " '[здоров]',\n",
              " '[что с тобой]',\n",
              " '[иортэ]',\n",
              " '[как]',\n",
              " '[ну как]',\n",
              " '[ну что ж]',\n",
              " '[ах да]',\n",
              " '[сударь]',\n",
              " '[утонул]',\n",
              " '[меня]',\n",
              " '[спросил он]',\n",
              " '[ну-ка]',\n",
              " '[увы]',\n",
              " '[но]',\n",
              " '[простите]',\n",
              " '[вот-вот]',\n",
              " '[быстро]',\n",
              " '[ждать]',\n",
              " '[зачем]',\n",
              " '[зачем]',\n",
              " '[по пути]',\n",
              " '[он здесь]',\n",
              " '[ив]',\n",
              " '[а другой]',\n",
              " '[да нет же]',\n",
              " '[случайно]',\n",
              " '[ну что ж]',\n",
              " '[а как же]',\n",
              " '[капри]',\n",
              " '[нет]',\n",
              " '[я просто]',\n",
              " '[уже день]',\n",
              " '[сонька]',\n",
              " '[ого]',\n",
              " '[э-э-э]',\n",
              " '[за что]',\n",
              " '[как]',\n",
              " '[а]',\n",
              " '[вот видишь]',\n",
              " '[боже]',\n",
              " '[а тебе]',\n",
              " '[кто]',\n",
              " '[не было]',\n",
              " '[спросил он]',\n",
              " '[не ври]',\n",
              " '[ладно]',\n",
              " '[крикнул гарри]',\n",
              " '[сказал я]',\n",
              " '[спросил я]',\n",
              " '[посмотри]',\n",
              " '[стрела]',\n",
              " '[ждешь чего]',\n",
              " '[нагнись]',\n",
              " '[когда]',\n",
              " '[а я]',\n",
              " '[марина]',\n",
              " '[спросил я]',\n",
              " '[ого]',\n",
              " '[а-а]',\n",
              " '[вот как]',\n",
              " '[у кого]',\n",
              " '[никогда]',\n",
              " '[а]',\n",
              " '[уф]',\n",
              " '[это реми]',\n",
              " '[я]',\n",
              " '[все]',\n",
              " '[да]',\n",
              " '[нет]',\n",
              " '[хорошо]',\n",
              " '[никто]',\n",
              " '[меня]',\n",
              " '[ничуть]',\n",
              " '[гальба]',\n",
              " '[а наши]',\n",
              " '[здесь]',\n",
              " '[штурм]',\n",
              " '[он придет]',\n",
              " '[почему]',\n",
              " '[был отлив]',\n",
              " '[ну что же]',\n",
              " '[насмерть]',\n",
              " '[чудо]',\n",
              " '[ты не спишь]',\n",
              " '[спросил он]',\n",
              " '[пару недель]',\n",
              " '[извини меня]',\n",
              " '[тише]',\n",
              " '[никогда]',\n",
              " '[ах]',\n",
              " '[молчите]',\n",
              " '[и что же]',\n",
              " '[спасибо]',\n",
              " '[оружием]',\n",
              " '[эпиграф]',\n",
              " '[не пойду]',\n",
              " '[а-а]',\n",
              " '[ничего]',\n",
              " '[отчего]',\n",
              " '[пойдемте же]',\n",
              " '[вы пешком]',\n",
              " '[устаете]',\n",
              " '[ну как]',\n",
              " '[хотя бы я]',\n",
              " '[а что]',\n",
              " '[я ушла]',\n",
              " '[зови]',\n",
              " '[потолковал]',\n",
              " '[зачем]',\n",
              " '[не знаю]',\n",
              " '[как]',\n",
              " '[туман]',\n",
              " '[татищев]',\n",
              " '[кто ушел]',\n",
              " '[а]',\n",
              " '[требуют]',\n",
              " '[нет]',\n",
              " '[да]',\n",
              " '[как так]',\n",
              " '[как]',\n",
              " '[так что]',\n",
              " '[ну]',\n",
              " '[вот как]',\n",
              " '[расписку]',\n",
              " '[риму]',\n",
              " '[уверена]',\n",
              " '[здесь]',\n",
              " '[подожди]',\n",
              " '[о боги]',\n",
              " '[гальба]',\n",
              " '[просто]',\n",
              " '[конечно]',\n",
              " '[я вижу]',\n",
              " '[здесь]',\n",
              " '[вот и все]',\n",
              " '[откуда]',\n",
              " '[или та]',\n",
              " '[гвион]',\n",
              " '[давно]',\n",
              " '[как же еще]',\n",
              " '[сюда]',\n",
              " '[и потянул]',\n",
              " '[иисус]',\n",
              " '[смотрите]',\n",
              " '[странно]',\n",
              " '[немедленно]',\n",
              " '[мяса]',\n",
              " '[стефан]',\n",
              " '[все они]',\n",
              " '[что с тобой]',\n",
              " '[ах да]',\n",
              " '[почти]',\n",
              " '[ну да]',\n",
              " '[хорошо]',\n",
              " '[ого]',\n",
              " '[спросил рон]',\n",
              " '[это трудно да]',\n",
              " '[зачем]',\n",
              " '[в пруде]',\n",
              " '[довольно]',\n",
              " '[кто он]',\n",
              " '[с сыном]',\n",
              " '[неврюй]',\n",
              " '[и решился]',\n",
              " '[отпевали]',\n",
              " '[как быть]',\n",
              " '[а]',\n",
              " '[в чем]',\n",
              " '[увы]',\n",
              " '[тоже]',\n",
              " '[увы]',\n",
              " '[шико]',\n",
              " '[ого]',\n",
              " '[да]',\n",
              " '[нет сударь]',\n",
              " '[наоборот]',\n",
              " '[бегите]',\n",
              " '[не бойся]',\n",
              " '[зачем]',\n",
              " '[смотри]',\n",
              " '[странно]',\n",
              " '[префекту]',\n",
              " '[валлиец]',\n",
              " '[да жаль]',\n",
              " '[бертрада]',\n",
              " '[слушаю]',\n",
              " '[все ясно]',\n",
              " '[и вот я тут]',\n",
              " '[ниниан]',\n",
              " '[может быть]',\n",
              " '[погоди-ка]',\n",
              " '[пожалуйста]',\n",
              " '[вульфила]',\n",
              " '[аврелий]',\n",
              " '[сюда]',\n",
              " '[нимало]',\n",
              " '[никогда]',\n",
              " '[спросил он]',\n",
              " '[время шло]',\n",
              " '[что элин]',\n",
              " '[я спрошу]',\n",
              " '[спросил он]',\n",
              " '[ну я пойду]',\n",
              " '[спросил он]',\n",
              " '[выпалил рон]',\n",
              " '[что это с ними]',\n",
              " '[очень мило]',\n",
              " '[не помню]',\n",
              " '[двести]',\n",
              " '[вон]',\n",
              " '[голямбек]',\n",
              " '[ступай]',\n",
              " '[зачем]',\n",
              " '[меня тоже]',\n",
              " '[эпилог]',\n",
              " '[но увы]',\n",
              " '[да да]',\n",
              " '[да верно]',\n",
              " '[так что же]',\n",
              " '[да я]',\n",
              " '[ого]',\n",
              " '[да]',\n",
              " '[браво]',\n",
              " '[да сир]',\n",
              " '[а]',\n",
              " '[откуда]',\n",
              " '[стену]',\n",
              " '[конечно]',\n",
              " '[закон]',\n",
              " '[сюда]',\n",
              " '[молчишь]',\n",
              " '[заходите]',\n",
              " '[вот и все]',\n",
              " '[пора]',\n",
              " '[никогда]',\n",
              " '[я тоже]',\n",
              " '[правильно]',\n",
              " '[мелангель]',\n",
              " '[и зачем]',\n",
              " '[ты уверена]',\n",
              " '[ну как]',\n",
              " '[вряд ли]',\n",
              " '[смотрите]',\n",
              " '[запомнил]',\n",
              " '[браво]',\n",
              " '[почему]',\n",
              " '[слышишь ли]',\n",
              " '[пока нет]',\n",
              " '[сегодня]',\n",
              " '[это я]',\n",
              " '[ты ли]',\n",
              " '[войны]',\n",
              " '[железо]',\n",
              " '[знаю]',\n",
              " '[третий]',\n",
              " '[что ж вы-то]',\n",
              " '[болен ты]',\n",
              " '[и отлично]',\n",
              " '[готово]',\n",
              " '[оно услышит]',\n",
              " '[гарри кивнул]',\n",
              " '[спросил рон]',\n",
              " '[спросил гарри]',\n",
              " '[мучаюсь]',\n",
              " '[плакала]',\n",
              " '[ладно]',\n",
              " '[татары]',\n",
              " '[четвертый]',\n",
              " '[суббота]',\n",
              " '[четвертая]',\n",
              " '[что там]',\n",
              " '[четвертое]',\n",
              " '[нисколько]',\n",
              " '[король]',\n",
              " '[хорошо]',\n",
              " '[ну что ж]',\n",
              " '[нет нет]',\n",
              " '[да сударь]',\n",
              " '[весь]',\n",
              " '[я]',\n",
              " '[тсс]',\n",
              " '[никола]',\n",
              " '[ого]',\n",
              " '[утро]',\n",
              " '[э]',\n",
              " '[но что]',\n",
              " '[подлецы]',\n",
              " '[правду]',\n",
              " '[о боги]',\n",
              " '[поехали]',\n",
              " '[шишки]',\n",
              " '[простите]',\n",
              " '[и что же]',\n",
              " '[предатель]',\n",
              " '[не время]',\n",
              " '[вот как]',\n",
              " '[и не проси]',\n",
              " '[кто это]',\n",
              " '[никогда]',\n",
              " '[господи]',\n",
              " '[спасибо]',\n",
              " '[а наутро]',\n",
              " '[верно]',\n",
              " '[взять их]',\n",
              " '[ты согласен]',\n",
              " '[спасибо]',\n",
              " '[британия]',\n",
              " '[не тревожься]',\n",
              " '[ничего]',\n",
              " '[слушайте]',\n",
              " '[не могу]',\n",
              " '[сюрприз]',\n",
              " '[что такое]',\n",
              " '[в чем дело]',\n",
              " '[никому]',\n",
              " '[может быть]',\n",
              " '[тренировка]',\n",
              " '[спросил вуд]',\n",
              " '[уизли]',\n",
              " '[ну и что]',\n",
              " '[а как же ты]',\n",
              " '[ты думаешь]',\n",
              " '[ты учил]',\n",
              " '[пятая]',\n",
              " '[миша встал]',\n",
              " '[что ж так]',\n",
              " '[на киев]',\n",
              " '[едем]',\n",
              " '[русские]',\n",
              " '[зачем]',\n",
              " '[кто идет]',\n",
              " '[кто таков]',\n",
              " '[тсс]',\n",
              " '[коня]',\n",
              " '[ну да]',\n",
              " '[вот как]',\n",
              " '[кто же он]',\n",
              " '[да]',\n",
              " '[никто]',\n",
              " '[хорошо]',\n",
              " '[потише]',\n",
              " '[да]',\n",
              " '[какие же]',\n",
              " '[проходите]',\n",
              " '[бель-эба]',\n",
              " '[а после]',\n",
              " '[никогда]',\n",
              " '[в лесу]',\n",
              " '[вот еще]',\n",
              " '[конечно]',\n",
              " '[о боги]',\n",
              " '[понимаешь]',\n",
              " '[он встал]',\n",
              " '[сможешь]',\n",
              " '[о боги]',\n",
              " '[а зачем]',\n",
              " '[оливье]',\n",
              " '[паренек]',\n",
              " '[он ранен]',\n",
              " '[это вы]',\n",
              " '[чудо]',\n",
              " '[он ушел]',\n",
              " '[амброзии]',\n",
              " '[и он тоже]',\n",
              " '[спросил он]',\n",
              " '[а павлин]',\n",
              " '[крикнула она]',\n",
              " '[нам пора]',\n",
              " '[он дышит]',\n",
              " '[тиран]',\n",
              " '[постой]',\n",
              " '[верно]',\n",
              " '[слушаюсь]',\n",
              " '[это тебе]',\n",
              " '[не поверил вуд]',\n",
              " '[тут близко]',\n",
              " '[парфяне]',\n",
              " '[море]',\n",
              " '[смотри-ка]',\n",
              " '[нет]',\n",
              " '[а]',\n",
              " '[ну как]',\n",
              " '[глядите]',\n",
              " '[есть будешь]',\n",
              " '[спрашиваю]',\n",
              " '[как нельзя]',\n",
              " '[э]',\n",
              " '[и тут же уснут]',\n",
              " '[привет гарри]',\n",
              " '[спросил гарри]',\n",
              " '[папаше]',\n",
              " '[добрый воин]',\n",
              " '[знаю]',\n",
              " '[давай давай]',\n",
              " '[куда]',\n",
              " '[и душно]',\n",
              " '[ядзя]',\n",
              " '[фу]',\n",
              " '[женщина]',\n",
              " '[сир]',\n",
              " '[откуда]',\n",
              " '[когда же]',\n",
              " '[ну]',\n",
              " '[ну да]',\n",
              " '[бегите]',\n",
              " '[а соус]',\n",
              " '[каком]',\n",
              " '[вот как]',\n",
              " '[нужен]',\n",
              " '[именно]',\n",
              " '[почему]',\n",
              " '[какие]',\n",
              " '[отвечай]',\n",
              " '[вот как]',\n",
              " '[замужем]',\n",
              " '[вителлий]',\n",
              " '[конечно]',\n",
              " '[и все же]',\n",
              " '[мейрион]',\n",
              " '[и не надо]',\n",
              " '[императора]',\n",
              " '[нежное]',\n",
              " '[например]',\n",
              " '[и трупы]',\n",
              " '[я ответил]',\n",
              " '[ты слышал]',\n",
              " '[кувшины]',\n",
              " '[давай сюда]',\n",
              " '[смотри]',\n",
              " '[катька]',\n",
              " '[смотрите]',\n",
              " '[сказала она]',\n",
              " '[гагарина]',\n",
              " '[не знаю]',\n",
              " '[никогда]',\n",
              " '[шутка]',\n",
              " '[что происходит]',\n",
              " '[подпишешь]',\n",
              " '[да что с вами]',\n",
              " '[рон]',\n",
              " '[это ещё кто]',\n",
              " '[оценил фред]',\n",
              " '[это вы]',\n",
              " '[входите]',\n",
              " '[спасли]',\n",
              " '[не помню]',\n",
              " '[подумал я]',\n",
              " '[как какое]',\n",
              " '[это я]',\n",
              " '[ты незнаком]',\n",
              " '[где я]',\n",
              " '[будет вам]',\n",
              " '[как его зовут]',\n",
              " '[следить]',\n",
              " '[да ну]',\n",
              " '[с детства]',\n",
              " '[увы]',\n",
              " '[ну да]',\n",
              " '[как]',\n",
              " '[сорок пять]',\n",
              " '[почти все]',\n",
              " '[чей приказ]',\n",
              " '[я готова]',\n",
              " '[а затем]',\n",
              " '[когда же]',\n",
              " '[почему]',\n",
              " '[вот как]',\n",
              " '[вот он]',\n",
              " '[да]',\n",
              " '[кто это]',\n",
              " '[занята]',\n",
              " '[после]',\n",
              " '[шутишь]',\n",
              " '[конечно]',\n",
              " '[три года]',\n",
              " '[я тоже]',\n",
              " '[о боги]',\n",
              " '[понимаешь]',\n",
              " '[пленники]',\n",
              " '[молчать]',\n",
              " '[о боги]',\n",
              " '[никогда]',\n",
              " '[да вот беда]',\n",
              " '[ушли]',\n",
              " '[удивился он]',\n",
              " '[нет]',\n",
              " '[берегись]',\n",
              " '[кто это]',\n",
              " '[но куда]',\n",
              " '[смотрите]',\n",
              " '[довольно]',\n",
              " '[но]',\n",
              " '[куда же еще]',\n",
              " '[я не знаю]',\n",
              " '[кому же]',\n",
              " '[да]',\n",
              " '[что это значит]',\n",
              " '[да но]',\n",
              " '[спросил шуц]',\n",
              " '[виктория]',\n",
              " '[не знаю]',\n",
              " '[ты здоров]',\n",
              " '[ха-ха-ха]',\n",
              " '[джеркони]',\n",
              " '[нет]',\n",
              " '[неправда]',\n",
              " '[не знаю]',\n",
              " '[гарри молчал]',\n",
              " '[какой гарри]',\n",
              " '[ликовал рон]',\n",
              " '[халдеи]',\n",
              " '[спросил гарри]',\n",
              " '[спросил гарри]',\n",
              " '[например]',\n",
              " '[сказала она]',\n",
              " '[ах да]',\n",
              " '[сказал я]',\n",
              " '[каким образом]',\n",
              " '[второе]',\n",
              " '[за русь]',\n",
              " '[держись]',\n",
              " '[обижал их]',\n",
              " '[кого ее]',\n",
              " '[что именно]',\n",
              " '[что]',\n",
              " '[второе]',\n",
              " '[крикнул он]',\n",
              " '[уехал]',\n",
              " '[увы]',\n",
              " '[о боже]',\n",
              " '[как так]',\n",
              " '[оружия]',\n",
              " '[короля]',\n",
              " '[я]',\n",
              " '[ага]',\n",
              " '[да сударь]',\n",
              " '[о чем]',\n",
              " '[и правда]',\n",
              " '[стражем]',\n",
              " '[не могу]',\n",
              " '[а я нет]',\n",
              " '[он спал]',\n",
              " '[и дружбу]',\n",
              " '[не знаю]',\n",
              " '[и зачем]',\n",
              " '[что тогда]',\n",
              " '[я здесь]',\n",
              " '[так ты идешь]',\n",
              " '[и я тоже]',\n",
              " '[ну и ладно]',\n",
              " '[спросил хью]',\n",
              " '[я боялся]',\n",
              " '[мы с вами]',\n",
              " '[отдохнуть]',\n",
              " '[поворачивай]',\n",
              " '[аквелия]',\n",
              " '[я его нашла]',\n",
              " '[я не устала]',\n",
              " '[ах ты]',\n",
              " '[пойдем ко мне]',\n",
              " '[где глэй]',\n",
              " '[вскрой его]',\n",
              " '[не понял гарри]',\n",
              " '[что происходит]',\n",
              " '[а]',\n",
              " '[э]',\n",
              " '[смотри-ка]',\n",
              " '[ну пошел]',\n",
              " '[ну так что ж]',\n",
              " '[в севске]',\n",
              " '[глава первая]',\n",
              " '[ври]',\n",
              " '[понимаете]',\n",
              " '[что это он]',\n",
              " '[сказал я]',\n",
              " '[тысячу]',\n",
              " '[нет нет]',\n",
              " '[только-то]',\n",
              " '[бредит]',\n",
              " '[знамение]',\n",
              " '[пошли миша]',\n",
              " '[ратмира]',\n",
              " '[подсоблю]',\n",
              " '[васса]',\n",
              " '[либо]',\n",
              " '[не шутишь]',\n",
              " '[он спасен]',\n",
              " '[увы]',\n",
              " '[что]',\n",
              " '[я-то]',\n",
              " '[их много]',\n",
              " '[ничего]',\n",
              " '[ты слышал]',\n",
              " '[хорошо]',\n",
              " '[лично]',\n",
              " '[я]',\n",
              " '[отлично]',\n",
              " '[но к чему]',\n",
              " '[какой язык]',\n",
              " '[нам пора]',\n",
              " '[гальба]',\n",
              " '[ага]',\n",
              " '[и кому же]',\n",
              " '[да один]',\n",
              " '[жена]',\n",
              " '[посмотри]',\n",
              " '[и мне жаль]',\n",
              " '[не забыл]',\n",
              " '[но почему]',\n",
              " '[я не знаю]',\n",
              " '[они выехали]',\n",
              " '[как я мог]',\n",
              " '[рабами]',\n",
              " '[но дальше]',\n",
              " '[где ты был]',\n",
              " '[наконец-то]',\n",
              " '[опять он]',\n",
              " '[батиат]',\n",
              " '[о да]',\n",
              " '[не знаю]',\n",
              " '[повторил он]',\n",
              " '[палашка]',\n",
              " '[это]',\n",
              " '[вампир]',\n",
              " '[они уехали]',\n",
              " '[вскричал он]',\n",
              " '[он помолчал]',\n",
              " '[абсолютно]',\n",
              " '[спросил шуц]',\n",
              " '[не знаю]',\n",
              " '[на меня]',\n",
              " '[конечно нет]',\n",
              " '[может флитвик]',\n",
              " '[это что такое]',\n",
              " '[сама не знаю]',\n",
              " '[воскликнул он]',\n",
              " '[охнул рон]',\n",
              " '[какой-то бред]',\n",
              " '[ну гарри]',\n",
              " '[как когда]',\n",
              " '[чего это]',\n",
              " '[напишу]',\n",
              " '[вкусно]',\n",
              " '[нет]',\n",
              " '[сбыслав]',\n",
              " '[ну ступай]',\n",
              " '[что]',\n",
              " '[неизвестно]',\n",
              " '[а]',\n",
              " '[как]',\n",
              " '[проститься]',\n",
              " '[да сударь]',\n",
              " '[что]',\n",
              " '[совет]',\n",
              " '[хорошо]',\n",
              " '[вот именно]',\n",
              " '[савия]',\n",
              " '[кровью]',\n",
              " '[вот именно]',\n",
              " '[а ворота]',\n",
              " '[но какой]',\n",
              " '[вначале]',\n",
              " '[или дети]',\n",
              " '[заходите]',\n",
              " '[спросил марк]',\n",
              " '[кто же она]',\n",
              " '[а что мальчик]',\n",
              " '[чего ради]',\n",
              " '[поспешите]',\n",
              " '[это приказ]',\n",
              " '[подождем]',\n",
              " '[прекрати]',\n",
              " '[очень мало]',\n",
              " '[я подожду]',\n",
              " '[что ты сказал]',\n",
              " '[пояснил он]',\n",
              " '[осторожнее]',\n",
              " '[записка]',\n",
              " '[ради чего]',\n",
              " '[что ж]',\n",
              " '[т-с-с-с]',\n",
              " '[ну]',\n",
              " '[что это]',\n",
              " '[отчего же]',\n",
              " '[воскликнул рон]',\n",
              " '[я пойду один]',\n",
              " '[удивился рон]',\n",
              " '[дороге]',\n",
              " '[государь]',\n",
              " '[максимыч]',\n",
              " '[спросил гарри]',\n",
              " '[а мы и не будем]',\n",
              " '[я его вижу]',\n",
              " '[завопил эльф]',\n",
              " '[третья]',\n",
              " '[действуй]',\n",
              " '[лук]',\n",
              " '[спросил яков]',\n",
              " '[четвертая]',\n",
              " '[не все ли равно]',\n",
              " '[так говори]',\n",
              " '[загадка]',\n",
              " '[беда]',\n",
              " '[уехал]',\n",
              " '[увы]',\n",
              " '[ужинать]',\n",
              " '[а почему]',\n",
              " '[ну вот]',\n",
              " '[спросил шико]',\n",
              " '[я влюблен]',\n",
              " '[вот как]',\n",
              " '[на суде]',\n",
              " '[почему]',\n",
              " '[ого]',\n",
              " '[правильно]',\n",
              " '[спросил шико]',\n",
              " '[ни за что]',\n",
              " '[варвары]',\n",
              " '[не думаю]',\n",
              " '[держись]',\n",
              " '[руби их]',\n",
              " '[это ложь]',\n",
              " '[три дня]',\n",
              " '[это верно]',\n",
              " '[ты его видел]',\n",
              " '[кадфаэль]',\n",
              " '[нет парень]',\n",
              " '[она умерла]',\n",
              " '[не сказал]',\n",
              " '[а она]',\n",
              " '[раз в жизни]',\n",
              " '[а сиаран]',\n",
              " '[он был один]',\n",
              " '[что именно]',\n",
              " '[неудача]',\n",
              " '[да]',\n",
              " '[как его зовут]',\n",
              " '[неправда]',\n",
              " '[аврелий]',\n",
              " '[а рукоятка]',\n",
              " '[добро]',\n",
              " '[одеваться]',\n",
              " '[да да]',\n",
              " '[от кого]',\n",
              " '[сказал он]',\n",
              " '[что же карета]',\n",
              " '[кричал он]',\n",
              " '[где оно]',\n",
              " '[аминь]',\n",
              " '[объясни]',\n",
              " '[нет]',\n",
              " '[известно]',\n",
              " '[да]',\n",
              " '[джеркони]',\n",
              " '[гмм]',\n",
              " '[и для чего]',\n",
              " '[но олдвин]',\n",
              " '[ну вот]',\n",
              " '[германец]',\n",
              " '[позвольте]',\n",
              " '[не надо]',\n",
              " '[да вот он]',\n",
              " '[кто это]',\n",
              " '[ну да]',\n",
              " '[и о народе]',\n",
              " '[здравствуй рон]',\n",
              " '[ведь любишь]',\n",
              " '[пора уж идти]',\n",
              " '[дурак]',\n",
              " '[как]',\n",
              " '[а ты]',\n",
              " '[отходим]',\n",
              " '[а сбыслав]',\n",
              " '[подумала она]',\n",
              " '[ты уж прости]',\n",
              " '[третье]',\n",
              " '[хорошая]',\n",
              " '[вот как]',\n",
              " '[тем лучше]',\n",
              " '[верно]',\n",
              " '[что же это]',\n",
              " '[сударыня]',\n",
              " '[зато]',\n",
              " '[дорогу]',\n",
              " '[ого]',\n",
              " '[кто он такой]',\n",
              " '[так точно]',\n",
              " '[вот именно]',\n",
              " '[конечно]',\n",
              " '[оно здесь]',\n",
              " '[да]',\n",
              " '[вульгарной]',\n",
              " '[матерью]',\n",
              " '[спасена]',\n",
              " '[римляне]',\n",
              " '[не думаю]',\n",
              " '[вроде вас]',\n",
              " '[врать]',\n",
              " '[так говорят]',\n",
              " '[твою]',\n",
              " '[ну вот и все]',\n",
              " '[у мельницы]',\n",
              " '[я хочу пить]',\n",
              " '[да конечно]',\n",
              " '[я его верну]',\n",
              " '[проклятие]',\n",
              " '[да а что]',\n",
              " '[ого]',\n",
              " '[прелестно]',\n",
              " '[а теперь]',\n",
              " '[ты ранен]',\n",
              " '[дрался]',\n",
              " '[отлично]',\n",
              " '[горелки]',\n",
              " '[крикнул кто-то]',\n",
              " '[погоди карл]',\n",
              " '[возможно]',\n",
              " '[вот это да]',\n",
              " '[на что это похоже]',\n",
              " '[вот те на]',\n",
              " '[вспылил фадж]',\n",
              " '[воскликнул гарри]',\n",
              " '[пойдём в спальню]',\n",
              " '[это не я]',\n",
              " '[говорил он]',\n",
              " '[израиль]',\n",
              " '[хамадан]',\n",
              " '[ок]',\n",
              " '[как же]',\n",
              " '[ты спишь]',\n",
              " '[вина]',\n",
              " '[спросил я]',\n",
              " '[бог миловал]',\n",
              " '[не отдам]',\n",
              " '[он вздохнул]',\n",
              " '[скучно там]',\n",
              " '[васса]',\n",
              " '[он не монгол]',\n",
              " '[ярун]',\n",
              " '[второе]',\n",
              " '[увы]',\n",
              " '[кончено]',\n",
              " '[да]',\n",
              " '[что я вижу]',\n",
              " '[видимо да]',\n",
              " '[котором]',\n",
              " '[вот как]',\n",
              " '[я тоже]',\n",
              " '[а вас]',\n",
              " '[из лувра]',\n",
              " '[как]',\n",
              " '[как сир]',\n",
              " '[вот как]',\n",
              " '[почему нет]',\n",
              " '[тем лучше]',\n",
              " '[отлично]',\n",
              " '[замолчи]',\n",
              " '[в тиранен]',\n",
              " '[не думаю]',\n",
              " '[пора спать]',\n",
              " '[прекрати]',\n",
              " '[неужели]',\n",
              " '[вот именно]',\n",
              " '[никогда]',\n",
              " '[превосходно]',\n",
              " '[но почему]',\n",
              " '[до сих пор]',\n",
              " '[да решено]',\n",
              " '[а мой сын]',\n",
              " '[можно войти]',\n",
              " '[ну мне пора]',\n",
              " '[конечно]',\n",
              " '[напротив]',\n",
              " '[ура]',\n",
              " '[кошмар]',\n",
              " '[передала она]',\n",
              " '[первая]',\n",
              " '[что внутри]',\n",
              " '[воскликнул рон]',\n",
              " '[что нам делать]',\n",
              " '[автомобиль]',\n",
              " '[фоукс]',\n",
              " '[как]',\n",
              " '[спросил он рона]',\n",
              " '[вы кто]',\n",
              " '[писал свиридов]',\n",
              " '[где побывал]',\n",
              " '[иди же]',\n",
              " '[пояснил он]',\n",
              " '[прислуга]',\n",
              " '[откуда]',\n",
              " '[условие первое]',\n",
              " '[джебе нойон]',\n",
              " '[понравилась]',\n",
              " '[бату вздохнул]',\n",
              " '[второе]',\n",
              " '[что именно]',\n",
              " '[ну как]',\n",
              " '[а]',\n",
              " '[прощай]',\n",
              " '[да брат]',\n",
              " '[а я-то]',\n",
              " '[как]',\n",
              " '[что вы]',\n",
              " '[увы]',\n",
              " '[но погоди]',\n",
              " '[в лувре]',\n",
              " '[что такое]',\n",
              " '[ну да]',\n",
              " '[ну и что же]',\n",
              " '[о отлично]',\n",
              " '[что было]',\n",
              " '[не сразу]',\n",
              " '[не знаю]',\n",
              " '[возможно]',\n",
              " '[на помощь]',\n",
              " '[о боги]',\n",
              " '[самайн]',\n",
              " '[немедленно]',\n",
              " '[ну да ладно]',\n",
              " '[и все же]',\n",
              " '[тут твой дом]',\n",
              " '[это правда]',\n",
              " '[а эти]',\n",
              " '[нет не верю]',\n",
              " '[скажи ей]',\n",
              " '[ее отцу]',\n",
              " '[брат кадфаэль]',\n",
              " '[верно]',\n",
              " '[убей меня]',\n",
              " '[я солгал]',\n",
              " '[что я там делал]',\n",
              " '[вот странно]',\n",
              " '[успокойся]',\n",
              " '[я буду там]',\n",
              " '[где ты был]',\n",
              " '[тише]',\n",
              " '[дай ему уйти]',\n",
              " '[ты как цел]',\n",
              " '[мост]',\n",
              " '[выдохнул он]',\n",
              " '[гельветы]',\n",
              " '[государь]',\n",
              " '[химера]',\n",
              " '[обещаюсь]',\n",
              " '[вот только]',\n",
              " '[я]',\n",
              " '[это ты]',\n",
              " '[из-за чего шум]',\n",
              " '[очень многое]',\n",
              " '[там стояли слова]',\n",
              " '[кто это]',\n",
              " '[точно так]',\n",
              " '[он дружески]',\n",
              " '[и не боялся]',\n",
              " '[как устроились]',\n",
              " '[и вышел]',\n",
              " '[того не может быть]',\n",
              " '[ярун вышел]',\n",
              " '[одна просьба]',\n",
              " '[спросил андрей]',\n",
              " '[ты уж прости]',\n",
              " '[четвертое]',\n",
              " '[кирдяш]',\n",
              " '[третья]',\n",
              " '[второе]',\n",
              " '[второе]',\n",
              " '[а между тем]',\n",
              " '[о да]',\n",
              " '[из ажана]',\n",
              " '[и что же]',\n",
              " '[реми]',\n",
              " '[едем туда]',\n",
              " '[я горю]',\n",
              " '[друзей]',\n",
              " '[да сир]',\n",
              " '[почему нет]',\n",
              " '[гизы]',\n",
              " '[ба]',\n",
              " '[так точно]',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6B2eCT8WgWD1"
      },
      "source": [
        "## Задание 4 (0.5 балла)\n",
        "Так как мы не располагаем большими мощностями, то давайте ограничим максимальную длинну текста. Вы можете менять этот порог и тем самым уменьшать кол-во текстов в вашей выборке и увеличивая тем самым скорость обучения. Начнем же мы с 128. \n",
        "Выберите порог и оставьте только те тексты, длина которых не превосходит данный порог.\n",
        "\n",
        "Далее разбейте тексты на train и test, перемешайте тексты при разбиении, размер тестовой выборки должен быть 15% от общего числа текстов. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oA7AXzsJgWD1"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QK3SR7vDgWD1"
      },
      "outputs": [],
      "source": [
        "THRESHOLD = 128\n",
        "for text in data[:]:\n",
        "    if len(text) > THRESHOLD:\n",
        "        data.remove(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ibB7n0WgWD1",
        "outputId": "bccb6214-fbb2-4c4a-d7a8-e7b7c1c25d16"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "683439"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "da9EewCOgWD2"
      },
      "outputs": [],
      "source": [
        "train, test = train_test_split(data, test_size=0.15, random_state=12345, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io7IJEX_gWD2"
      },
      "source": [
        "## Задание 5 (2 балла)\n",
        "Напишем датасет. На вход датасету передается набор текстов, объект класса Preprocessor и размер окна, который вы выбрали в прошлом задании.\n",
        "Реализуйте методы __len__ и __getitem__."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "h5CGfm0CgWD2"
      },
      "outputs": [],
      "source": [
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    \n",
        "    def __init__(self, x, preproc, win_size = 128):\n",
        "        self.x = x\n",
        "        self.preproc = preproc\n",
        "        self.win_size = win_size\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return preproc.preprocess(self.x[idx], self.win_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ty4-onmygWD2"
      },
      "outputs": [],
      "source": [
        "preproc = Preprocessor()\n",
        "train_dataset = TextDataset(train, preproc)\n",
        "test_dataset = TextDataset(test, preproc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4uRaJFZgWD2"
      },
      "source": [
        "## Задание 6 (2 балла)\n",
        "Напишем модель. Класс для реализации positional encoding реализован за вас, он нужен, чтобы модель могла после получения эмбедингов понимать, на каком месте какой токен находится.\n",
        "\n",
        "Заполните пропуски в классе модели. Гипперпараметры модели вам предлагается подобрать самостоятельно. Рекомендуется использовать не более 6 слоев в трансформере. В декореде испоьлзуйте две линейных слоя с функцией активации ReLU между ними.\n",
        "\n",
        "## Задание 6_1 (0 баллов, но надо ответить!)\n",
        "При обучении языковой модели на основе трансформеров мы используем маскирование символов (как мы это делаем - уже реализовано). Напишите, почему мы это делаем? Почему это так важно?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "С помощью маски мы делаем так, что модель не подглядывает в ответы, то есть на текущем символе мы не видим то, что будет после него. И за счет этого мы смотрим только на то, что было раньше до этого символа, и таким образом обучаемся."
      ],
      "metadata": {
        "id": "cvKwQyStYoom"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "92Y3AuSHgWD3"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "6M5EppF4gWD3"
      },
      "outputs": [],
      "source": [
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_heads):\n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.d_model = d_model\n",
        "        self.pe = PositionalEncoding(d_model)\n",
        "        self.transformer_encoder_layer = nn.TransformerEncoderLayer(d_model, n_heads)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_encoder_layer, num_layers=5) \n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(d_model, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, vocab_size))\n",
        "    \n",
        "    def forward(self, x, src_mask): #src_mask\n",
        "        x = self.pe(self.emb(x))\n",
        "        x = x.transpose(1, 0)\n",
        "        x = self.transformer_encoder(x, src_mask) # transformer encoder with mask\n",
        "        x = self.decoder(x)\n",
        "        return x.transpose(1, 0)\n",
        "    \n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        # А вот и то самое маскирование\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Zy6nYYHfgWD3"
      },
      "outputs": [],
      "source": [
        "model = LanguageModel(len('_добсркгаупитнезчмфяжлйвцыэь-шхющёъ][ '), 256, 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3kAxNuzgWD3"
      },
      "source": [
        "## Задание 7 (2,5 балла)\n",
        "Финишная прямая. Давайте реализуем класс для обучения модели и ее валидации. Следуйте указаниям в коде и заполните недостающие фрагменты в коде."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "qx1Cz0pZgWD3"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    \n",
        "    def __init__(self, model, train_dataset, test_dataset):\n",
        "        \n",
        "        self.model = model\n",
        "        \n",
        "        self.train_batch_size = 64\n",
        "        self.test_batch_size = 64\n",
        "        \n",
        "        self.train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=self.train_batch_size)\n",
        "        self.test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=self.test_batch_size)\n",
        "        self.train_dataloader_size = len(self.train_dataloader)\n",
        "        self.test_dataloader_size = len(self.test_dataloader)\n",
        "        \n",
        "        self.device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "        self.criterion = nn.CrossEntropyLoss(ignore_index=0) # используйте CrossEntrophyLoss, передайте в качетсве параметра \n",
        "                             # ignore index индекс символа _, чтобы модель не штрафовалась за то\n",
        "                             # что идет после закрывающего токена\n",
        "        \n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001) # lr=0.01\n",
        "        \n",
        "        self.steps_to_print = 250\n",
        "        \n",
        "    def train_one_epoch(self, epoch_number):\n",
        "        step = 0\n",
        "        counted_loss = 0\n",
        "        current_time = time.time()\n",
        "        it = 0\n",
        "        \n",
        "        for batch in tqdm(self.train_dataloader):\n",
        "            x, y = batch\n",
        "            # YOUR CODE HERE\n",
        "            it += 1\n",
        "            step += 1\n",
        "\n",
        "            x = x.to(self.device) \n",
        "            y = y.to(self.device) \n",
        "\n",
        "            sequence_length = x.size(1)\n",
        "            src_mask = self.model.generate_square_subsequent_mask(sequence_length).to(self.device)\n",
        "            predicted = self.model(x, src_mask)\n",
        "\n",
        "            loss = self.criterion(predicted.permute(0, 2, 1), y)\n",
        "\n",
        "            # Update weights\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            self.optimizer.zero_grad()\n",
        "            \n",
        "            counted_loss += loss.detach().item()\n",
        "            # реализуйте шаги обучения модели\n",
        "            # сохраняйте значение ошибки в переменную counted_loss\n",
        "            \n",
        "            ################\n",
        "            \n",
        "            \n",
        "            if step%self.steps_to_print == 0:\n",
        "                result = 'Train epoch '+str(epoch_number)+' | '\n",
        "                result += 'Step '+str(step)+'/'+str(self.train_dataloader_size)+' | '\n",
        "                result += 'Counted loss '+str(counted_loss/self.steps_to_print)+' | '\n",
        "                result += 'ppl '+str(math.exp(counted_loss/it))+' | '\n",
        "                result += 'time '+str(time.time() - current_time) + ' | '\n",
        "                print(result)\n",
        "                current_time = time.time()\n",
        "                counted_loss = 0\n",
        "                it = 0\n",
        "    \n",
        "    def validate_one_epoch(self, epoch_number):\n",
        "        step = 0\n",
        "        counted_loss = 0\n",
        "        current_time = time.time()\n",
        "        it = 0\n",
        "        for batch in tqdm(self.test_dataloader):\n",
        "            x, y = batch\n",
        "            step += 1\n",
        "            it += 1\n",
        "            # YOUR CODE HERE\n",
        "            \n",
        "            # реализуйте шаги для теста модели\n",
        "            # помните, что данный метод уже запускается из \n",
        "            # блока with torch.no_grad(), а потому \n",
        "            # повторно его использовать не нужно\n",
        "            \n",
        "            x = x.to(self.device)\n",
        "            y = y.to(self.device)\n",
        "            sequence_length = y.size(1)\n",
        "            src_mask = self.model.generate_square_subsequent_mask(sequence_length).to(self.device)\n",
        "            predicted = self.model(x, src_mask)\n",
        "            loss = self.criterion(predicted.permute(0, 2, 1), y)\n",
        "            counted_loss += loss.item()\n",
        "            \n",
        "            ################\n",
        "            \n",
        "            if step%(self.steps_to_print//2) == 0:\n",
        "                result = 'Validate epoch '+str(epoch_number)+' | '\n",
        "                result += 'Step '+str(step)+'/'+str(self.test_dataloader_size)+' | '\n",
        "                result += 'Counted loss '+str(counted_loss/self.steps_to_print)+' | '\n",
        "                result += 'ppl '+str(math.exp(counted_loss/it))+' | '\n",
        "                result += 'time '+str(time.time() - current_time) + ' | '\n",
        "                print(result)\n",
        "                current_time = time.time()\n",
        "                counted_loss = 0\n",
        "                it = 0\n",
        "        \n",
        "    def train(self, number_of_epochs):\n",
        "        model.to(self.device)\n",
        "        for epoch in range(1, number_of_epochs+1):\n",
        "            model.train()\n",
        "            self.train_one_epoch(epoch)\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                self.validate_one_epoch(epoch)\n",
        "            print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_eJXmi6gWD4"
      },
      "source": [
        "Что такое ppl? Перплексия. Ее можно интерпретировать как меру \"удивленности\" модели нужному символу. Чем меньше данная величина, тем лучше, ведь это значит, что модель если и сделала неправильный выбор, то не сильно удивлена своей ошибке.\n",
        "\n",
        "Проведите несколько экспериментов, посмотрите, при каких гипперпараметрах значение перплексии минимально."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPJkyJFXgWD5"
      },
      "source": [
        "## Задание 8 (0.5 балла)\n",
        "Запустите обучение на нескольких эпохах. Ориентируйтесь на ваши вычислительные мощности и время работы. Вы всегда можете посчитать, сколько секунд уходит на один батч."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zOaC7QeHgWD6",
        "outputId": "1c1f865c-c230-4007-d0ce-103506ee5d10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 250/9077 [01:10<41:11,  3.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 | Step 250/9077 | Counted loss 2.547689658164978 | ppl 12.777549148808081 | time 70.6369137763977 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 500/9077 [02:20<40:36,  3.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 | Step 500/9077 | Counted loss 1.9761751217842103 | ppl 7.215093287366765 | time 70.23758053779602 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|▊         | 750/9077 [03:31<38:50,  3.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 | Step 750/9077 | Counted loss 1.7502628974914551 | ppl 5.756115745495945 | time 70.16983151435852 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 11%|█         | 1000/9077 [04:41<37:44,  3.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 | Step 1000/9077 | Counted loss 1.652072031021118 | ppl 5.217780036228137 | time 70.04502630233765 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 14%|█▍        | 1250/9077 [05:51<36:33,  3.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 | Step 1250/9077 | Counted loss 1.58943843126297 | ppl 4.900995909311284 | time 70.25210046768188 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 1500/9077 [07:01<35:25,  3.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 | Step 1500/9077 | Counted loss 1.5489892954826354 | ppl 4.706710684022282 | time 70.13771033287048 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 19%|█▉        | 1750/9077 [08:11<34:14,  3.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 | Step 1750/9077 | Counted loss 1.5276560220718385 | ppl 4.607364594522003 | time 70.13895058631897 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 22%|██▏       | 2000/9077 [09:21<33:13,  3.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 | Step 2000/9077 | Counted loss 1.494057951927185 | ppl 4.45513762155028 | time 70.13720107078552 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▍       | 2250/9077 [10:31<31:49,  3.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 | Step 2250/9077 | Counted loss 1.4786597328186035 | ppl 4.387061903790418 | time 70.05511498451233 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 28%|██▊       | 2500/9077 [11:41<30:57,  3.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 | Step 2500/9077 | Counted loss 1.4706291542053223 | ppl 4.351972341612004 | time 70.152911901474 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 2750/9077 [12:52<29:30,  3.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 | Step 2750/9077 | Counted loss 1.4521010599136353 | ppl 4.272080990436244 | time 70.14060735702515 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 33%|███▎      | 3000/9077 [14:02<28:20,  3.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 | Step 3000/9077 | Counted loss 1.4405878524780273 | ppl 4.223177692908554 | time 70.04150652885437 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 36%|███▌      | 3250/9077 [15:12<27:08,  3.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 | Step 3250/9077 | Counted loss 1.427848087310791 | ppl 4.1697166643815615 | time 70.10697817802429 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 39%|███▊      | 3500/9077 [16:22<26:14,  3.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 | Step 3500/9077 | Counted loss 1.4125287470817567 | ppl 4.106326146311953 | time 70.11322498321533 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 41%|████▏     | 3750/9077 [17:32<24:57,  3.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 | Step 3750/9077 | Counted loss 1.4175797600746154 | ppl 4.127119723134846 | time 70.13346934318542 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 44%|████▍     | 4000/9077 [18:42<23:44,  3.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 | Step 4000/9077 | Counted loss 1.4122520360946655 | ppl 4.105190037944772 | time 70.15401363372803 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 47%|████▋     | 4250/9077 [19:52<22:26,  3.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 | Step 4250/9077 | Counted loss 1.3874671478271485 | ppl 4.004693898762058 | time 69.95680475234985 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|████▉     | 4500/9077 [21:02<21:20,  3.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 | Step 4500/9077 | Counted loss 1.3864053502082825 | ppl 4.000443980991634 | time 70.10188627243042 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 52%|█████▏    | 4750/9077 [22:12<20:11,  3.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 | Step 4750/9077 | Counted loss 1.38630016040802 | ppl 4.0000231972197815 | time 70.03648948669434 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 55%|█████▌    | 5000/9077 [23:22<19:00,  3.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 | Step 5000/9077 | Counted loss 1.3849749097824096 | ppl 3.9947256750228437 | time 70.03302669525146 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 58%|█████▊    | 5250/9077 [24:32<17:50,  3.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 | Step 5250/9077 | Counted loss 1.381938199043274 | ppl 3.982613248940788 | time 70.02842378616333 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 61%|██████    | 5500/9077 [25:42<16:53,  3.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 | Step 5500/9077 | Counted loss 1.3663013262748718 | ppl 3.9208220022648144 | time 70.04686331748962 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 63%|██████▎   | 5750/9077 [26:52<15:33,  3.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 | Step 5750/9077 | Counted loss 1.358472972393036 | ppl 3.890248247347668 | time 70.04145216941833 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 66%|██████▌   | 6000/9077 [28:03<14:23,  3.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 | Step 6000/9077 | Counted loss 1.363703254699707 | ppl 3.9106486473440625 | time 70.14144897460938 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 69%|██████▉   | 6250/9077 [29:13<13:10,  3.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 | Step 6250/9077 | Counted loss 1.3559636669158937 | ppl 3.88049866357092 | time 70.05576205253601 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 72%|███████▏  | 6500/9077 [30:23<12:05,  3.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 | Step 6500/9077 | Counted loss 1.3528554573059082 | ppl 3.8684559856122784 | time 69.97920894622803 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|███████▍  | 6750/9077 [31:33<10:53,  3.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 | Step 6750/9077 | Counted loss 1.3482550492286682 | ppl 3.8507003822747174 | time 70.08834767341614 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 77%|███████▋  | 7000/9077 [32:43<09:43,  3.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 | Step 7000/9077 | Counted loss 1.3469806275367737 | ppl 3.8457960919093477 | time 70.14931297302246 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|███████▉  | 7250/9077 [33:53<08:36,  3.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 | Step 7250/9077 | Counted loss 1.3375079851150513 | ppl 3.8095382406543536 | time 70.10007333755493 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 83%|████████▎ | 7500/9077 [35:03<07:20,  3.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 | Step 7500/9077 | Counted loss 1.335882550239563 | ppl 3.803351114086358 | time 70.0891478061676 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 85%|████████▌ | 7750/9077 [36:13<06:12,  3.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 | Step 7750/9077 | Counted loss 1.3331295022964478 | ppl 3.7928947062251845 | time 70.05428266525269 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 88%|████████▊ | 8000/9077 [37:23<05:04,  3.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 | Step 8000/9077 | Counted loss 1.3363324174880982 | ppl 3.8050625021071123 | time 70.13640022277832 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 91%|█████████ | 8250/9077 [38:33<03:51,  3.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 | Step 8250/9077 | Counted loss 1.3224059805870056 | ppl 3.752438819969614 | time 70.12078809738159 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 94%|█████████▎| 8500/9077 [39:43<02:41,  3.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 | Step 8500/9077 | Counted loss 1.3282988891601561 | ppl 3.774616881325724 | time 70.12571048736572 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 96%|█████████▋| 8750/9077 [40:54<01:31,  3.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 | Step 8750/9077 | Counted loss 1.3246499876976012 | ppl 3.760868774264924 | time 70.09976506233215 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 99%|█████████▉| 9000/9077 [42:04<00:21,  3.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch 1 | Step 9000/9077 | Counted loss 1.3208073539733887 | ppl 3.746444863731539 | time 70.05885291099548 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9077/9077 [42:25<00:00,  3.57it/s]\n",
            "  8%|▊         | 126/1602 [00:13<02:38,  9.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validate epoch 1 | Step 125/1602 | Counted loss 0.6336278886795044 | ppl 3.5510941858355625 | time 13.261018514633179 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 16%|█▌        | 251/1602 [00:26<02:22,  9.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validate epoch 1 | Step 250/1602 | Counted loss 0.6255381917953491 | ppl 3.4941019280313528 | time 13.222458839416504 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 23%|██▎       | 376/1602 [00:39<02:09,  9.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validate epoch 1 | Step 375/1602 | Counted loss 0.6323775062561036 | ppl 3.542224829038073 | time 13.254699945449829 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 31%|███▏      | 501/1602 [00:53<01:55,  9.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validate epoch 1 | Step 500/1602 | Counted loss 0.6283106842041015 | ppl 3.5135304859218004 | time 13.258720397949219 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 39%|███▉      | 626/1602 [01:06<01:43,  9.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validate epoch 1 | Step 625/1602 | Counted loss 0.630555552482605 | ppl 3.529340777652686 | time 13.205662727355957 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 47%|████▋     | 751/1602 [01:19<01:31,  9.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validate epoch 1 | Step 750/1602 | Counted loss 0.6320782794952392 | ppl 3.540105606305783 | time 13.234960556030273 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 55%|█████▍    | 876/1602 [01:32<01:17,  9.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validate epoch 1 | Step 875/1602 | Counted loss 0.6283019680976868 | ppl 3.513469237844433 | time 13.279229879379272 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 62%|██████▏   | 1001/1602 [01:46<01:04,  9.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validate epoch 1 | Step 1000/1602 | Counted loss 0.6335926370620728 | ppl 3.5508438310336725 | time 13.220517635345459 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 1126/1602 [01:59<00:50,  9.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validate epoch 1 | Step 1125/1602 | Counted loss 0.6257464437484741 | ppl 3.495557538245841 | time 13.229058504104614 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 78%|███████▊  | 1251/1602 [02:12<00:36,  9.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validate epoch 1 | Step 1250/1602 | Counted loss 0.6268892288208008 | ppl 3.5035560172546947 | time 13.24284553527832 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 86%|████████▌ | 1376/1602 [02:25<00:23,  9.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validate epoch 1 | Step 1375/1602 | Counted loss 0.630346598625183 | ppl 3.5278661470646036 | time 13.22812271118164 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 94%|█████████▎| 1501/1602 [02:38<00:10,  9.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validate epoch 1 | Step 1500/1602 | Counted loss 0.6260684146881104 | ppl 3.4978091990276554 | time 13.218628406524658 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1602/1602 [02:49<00:00,  9.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 18/9077 [00:05<44:32,  3.39it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-c95ce955b2e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-43-70ad75f8bb1d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, number_of_epochs)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_epochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-70ad75f8bb1d>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(self, epoch_number)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;31m# Update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer = Trainer(model, train_dataset, test_dataset)\n",
        "\n",
        "trainer.train(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Так как модель обучается долго, а ошибка почти не падает, то я прервал обучение после 1й эпохи. Несмотря на это, она уже выдает более менее осмысленные тексты (видно дальше)"
      ],
      "metadata": {
        "id": "CG8U3x1ITmg0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXNJVtSRgWD6"
      },
      "source": [
        "## Задание 9 (1 балл)\n",
        "Итак, давайте попробуем погенерировать текст нашей сеткой. Закончите функцию по генерации текста. Попробуйте сгенерировать какой-нибудь текст. Помните, что если вы хотите генерировать текст с нуля, то вы должны передать в качестве текста только токен start.\n",
        "Прекратите генерировать текст, если модель выдала токен end или длинна текста больше 150."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "60vW3cglymRo"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Максимальную длинну текста пришлось ограничить до 50, так как при большей длине кончается память на гпу из-за большого количество вызовов рекуррентной функции генерации текста"
      ],
      "metadata": {
        "id": "y3_E5siVVKFV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "qITB6bHpgWD6"
      },
      "outputs": [],
      "source": [
        "def generate_text(text):\n",
        "    x = []\n",
        "    for letter in text:\n",
        "        x.append(preproc.token2ind[letter])\n",
        "    x = torch.from_numpy(np.array(x)).to(device)\n",
        "    sequence_length = len(x)\n",
        "    src_mask = model.generate_square_subsequent_mask(sequence_length).to(device)\n",
        "\n",
        "    pred = model(x, src_mask)\n",
        "    ind = pred.topk(1)[1].view(-1)[-1].item()\n",
        "    \n",
        "    text += preproc.ind2token[ind]\n",
        "    \n",
        "    if len(text) > 50 or text[-1] == \"]\":\n",
        "        return text\n",
        "    else:\n",
        "        return generate_text(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text('[здравствуйте')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "MuQPcwlL11ha",
        "outputId": "f4403c39-0e78-4af5-c9a8-f98f3bbc2281"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[здравствуйте меня зовут антон слушаю вас]'"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text('[')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "YfqOWbaXUAfG",
        "outputId": "5e414818-974e-4b3f-f266-c5b66aae5aed"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[ну вот так сейчас я посмотрю вот вот так по поводу'"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text('[глубинное обучение')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "jVTPinG2UmAF",
        "outputId": "46b1d653-14df-48f8-d820-89a2ee67b3d7"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[глубинное обучение по поводу подробнее подразделен'"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text('[до свидания')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "MhP3ZA8VVa13",
        "outputId": "6f38ed66-8a8a-40ed-931f-5fe5fa2810e0"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[до свидания спасибо до свидания]'"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csNnhg0jgWD7"
      },
      "source": [
        "## Задание 10* (Задание - бонус, 5 баллов за реализацию при условии, что сделаны прошлые задания)\n",
        "Давайте вспомним, что такое transfer learning. Мы хотим использовать уже предобученные эмбединги для нашей сети, чтобы наша сеть обучалась быстрее. Давайте попробуем обучить новую модель на уровне слов, а не символов, но для упрощения задачи используем предобученный слой из библиотеки Natasha, а вернее, ее блок Navec."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrDAsSF3gWD7"
      },
      "source": [
        "[Изучите](https://github.com/natasha/navec) то, как вставить слой в вашу нейронную сеть."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3llI9jdgWD7"
      },
      "source": [
        "Теперь мы хотим, чтобы на вход модели подавались слова, модифицируйте ваш датасет. Возвращайте теперь номер слова в словаре navec."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3zvb8VngWD7"
      },
      "outputs": [],
      "source": [
        "class TextDataset_Navec(torch.utils.data.Dataset):\n",
        "    \n",
        "    def __init__(self, x, win_size = 128):\n",
        "        # YOUR CODE HERE\n",
        "        self.navec = ...\n",
        "        ################\n",
        "    \n",
        "    def __len__(self):\n",
        "        # YOUR CODE HERE\n",
        "        ################\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # YOUR CODE HERE\n",
        "        ################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3byDakAgWD8"
      },
      "source": [
        "Немного модифицируем модель. Теперь нам не нужны слои с трансформером, так как весь механизм внимания уже заложен в ембедингах. Давайте попробуем просто пройтись линейной головой над эмбедингами. Выберите параметры самостоятельно."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42kWx55AgWD8"
      },
      "outputs": [],
      "source": [
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.emb_navec = ...\n",
        "        self.head = ...\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = ... # emb\n",
        "        x = ... # head\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwCP6FjYgWD9"
      },
      "source": [
        "Теперь дело за малым! Надо немного модифицировать класс обучения, так как мы не используем маскирование, после чего можно приступить к тесту!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoI1h2WwgWD9"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    \n",
        "    def __init__(self, model, train_dataset, test_dataset):\n",
        "        \n",
        "        self.model = model\n",
        "        \n",
        "        self.train_batch_size = 64\n",
        "        self.test_batch_size = 64\n",
        "        \n",
        "        self.train_dataloader = ...\n",
        "        self.test_dataloader = ...\n",
        "        self.train_dataloader_size = ...\n",
        "        self.test_dataloader_size = ...\n",
        "        \n",
        "        self.device = 'cuda:0'\n",
        "        self.criterion = ... \n",
        "        \n",
        "        self.optimizer = ...\n",
        "        \n",
        "        self.steps_to_print = 1000\n",
        "        \n",
        "    def train_one_epoch(self, epoch_number):\n",
        "        step = 0\n",
        "        counted_loss = 0\n",
        "        current_time = time.time()\n",
        "        it = 0\n",
        "        \n",
        "        for batch in self.train_dataloader:\n",
        "            x, y = batch\n",
        "            # YOUR CODE HERE\n",
        "            \n",
        "            # реализуйте шаги обучения модели\n",
        "            # сохраняйте значение ошибки в переменную counted_loss\n",
        "            \n",
        "            ################\n",
        "            \n",
        "            \n",
        "            if step%self.steps_to_print == 0:\n",
        "                result = 'Train epoch '+str(epoch_number)+' | '\n",
        "                result += 'Step '+str(step)+'/'+str(self.train_dataloader_size)+' | '\n",
        "                result += 'Counted loss '+str(counted_loss)+' | '\n",
        "                result += 'ppl '+str(math.exp(counted_loss/it))+' | '\n",
        "                result += 'time '+str(time.time() - current_time) + ' | '\n",
        "                print(result)\n",
        "                current_time = time.time()\n",
        "                counted_loss = 0\n",
        "                it = 0\n",
        "    \n",
        "    def validate_one_epoch(self, epoch_number):\n",
        "        step = 0\n",
        "        counted_loss = 0\n",
        "        current_time = time.time()\n",
        "        it = 0\n",
        "        for batch in self.test_dataloader:\n",
        "            x, y = batch\n",
        "            \n",
        "            # YOUR CODE HERE\n",
        "            \n",
        "            # реализуйте шаги для теста модели\n",
        "            # помните, что данный метод уже запускается из \n",
        "            # блока with torch.no_grad(), а потому \n",
        "            # повторно его использовать не нужно\n",
        "            \n",
        "            ################\n",
        "            \n",
        "            if step%(self.steps_to_print//2) == 0:\n",
        "                result = 'Validate epoch '+str(epoch_number)+' | '\n",
        "                result += 'Step '+str(step)+'/'+str(self.test_dataloader_size)+' | '\n",
        "                result += 'Counted loss '+str(counted_loss)+' | '\n",
        "                result += 'ppl '+str(math.exp(counted_loss/it))+' | '\n",
        "                result += 'time '+str(time.time() - current_time) + ' | '\n",
        "                print(result)\n",
        "                current_time = time.time()\n",
        "                counted_loss = 0\n",
        "                it = 0\n",
        "        \n",
        "    def train(self, number_of_epochs):\n",
        "        model.to(self.device)\n",
        "        for epoch in range(1, number_of_epochs+1):\n",
        "            model.train()\n",
        "            self.train_one_epoch(epoch)\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                self.validate_one_epoch(epoch)\n",
        "            print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwfcVC-DgWD9"
      },
      "source": [
        "Запустите обучение. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYi5l93CgWD9"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "###############"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "hw3_transformer_3-2.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}